{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Extract Summit Local-Chapter India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "uv venv --python 3.11\n",
    "source .venv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above commands:  \n",
    "Installs `uv` -> Installs python (if not present) -> creates and activates venv -> installs requirements  \n",
    "pip can be used for this too, uv is faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from math import ceil\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from curl_cffi import requests as curl_cffi_requests\n",
    "from IPython.display import Image, display\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm.auto import tqdm\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's explore web scraping by building something useful - a tool that tracks shoe prices across different websites\n",
    "- On Nike's website, like most e-commerce platforms, products are organized in a hierarchy - similar to folders on your computer. At the most detailed level, we have individual products like \"Air Jordan 1 High OG.\"\n",
    "- These products are grouped into subcategories (like \"Basketball Shoes\" or \"Running Shoes\"), which are then organized under broader categories (like \"Men's Shoes\" or \"Women's Shoes\") - making it easier to navigate and scrape systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Extracting nike product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_url = (\n",
    "    \"https://www.nike.com/in/t/zoom-fly-6-road-running-shoes-943Pvv/HV4366-072\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"priority\": \"u=0, i\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    product_url,\n",
    "    headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Web Page] --> B{Check Response Type}\n",
    "    B -->|HTML| C[Use XPath/CSS Selectors]\n",
    "    B -->|JSON| D[Use JPath/Python Dict]\n",
    "    B -->|Mixed| E[HTML with Embedded JSON]\n",
    "    E --> F[Extract JSON using BS4]\n",
    "    F --> G[Parse Extracted JSON]\n",
    "    \n",
    "    C -->|Extract Data| H[Final Data]\n",
    "    D -->|Extract Data| H\n",
    "    G -->|Extract Data| H    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_next_data(html_content):\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find the script tag with id=\"__NEXT_DATA__\"\n",
    "    next_data_script = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "\n",
    "    if next_data_script:\n",
    "        # Extract and parse the JSON content\n",
    "        json_data = json.loads(next_data_script.string)\n",
    "        return json_data\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "data = extract_next_data(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_nike_product_info(data):\n",
    "    \"\"\"\n",
    "    Extract key information from Nike product JSON data.\n",
    "\n",
    "    Args:\n",
    "        data: dict containing Nike product data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted product information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Navigate to the product data\n",
    "        product_data = data[\"props\"][\"pageProps\"][\"selectedProduct\"]\n",
    "\n",
    "        # Extract basic product information\n",
    "        extracted_info = {\n",
    "            \"title\": product_data[\"productInfo\"][\"title\"],\n",
    "            \"subtitle\": product_data[\"productInfo\"][\"subtitle\"],\n",
    "            \"full_title\": product_data[\"productInfo\"][\"fullTitle\"],\n",
    "            \"description\": product_data[\"productInfo\"][\"productDescription\"],\n",
    "            \"color_description\": product_data[\"colorDescription\"],\n",
    "            \"style_code\": product_data[\"styleCode\"],\n",
    "            \"price\": {\n",
    "                \"current\": product_data[\"prices\"][\"currentPrice\"],\n",
    "                \"currency\": product_data[\"prices\"][\"currency\"],\n",
    "            },\n",
    "            \"country_origin\": product_data[\"manufacturingCountriesOfOrigin\"][0],\n",
    "            \"available_sizes\": [],\n",
    "        }\n",
    "\n",
    "        # Extract available sizes\n",
    "        for size in product_data[\"sizes\"]:\n",
    "            size_info = {\n",
    "                \"label\": size[\"label\"],\n",
    "                \"localized_label\": size[\"localizedLabel\"],\n",
    "                \"prefix\": size[\"localizedLabelPrefix\"],\n",
    "                \"status\": size[\"status\"],\n",
    "            }\n",
    "            extracted_info[\"available_sizes\"].append(size_info)\n",
    "\n",
    "        # Extract benefits\n",
    "        benefits = []\n",
    "        for feature in product_data[\"productInfo\"][\"featuresAndBenefits\"]:\n",
    "            if feature[\"header\"] == \"Benefits\":\n",
    "                benefits = feature[\"body\"]\n",
    "        extracted_info[\"benefits\"] = benefits\n",
    "\n",
    "        # Extract product details\n",
    "        details = []\n",
    "        for detail_section in product_data[\"productInfo\"][\"productDetails\"]:\n",
    "            if detail_section[\"header\"] == \"Product details\":\n",
    "                details = detail_section[\"body\"]\n",
    "        extracted_info[\"product_details\"] = details\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting data: {str(e)}\"\n",
    "\n",
    "\n",
    "product_info = extract_nike_product_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"nike_product.json\", \"w\") as f:\n",
    "    json.dump(product_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Extracting nike subcategory and category details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most modern websites like Nike load their product data through APIs, but they hide API parameters (like concept_ids) in their HTML page for security. This two-step approach of first visiting the HTML page to extract these \"secret keys\" helps prevent unauthorized bulk scraping and API abuse.\n",
    "\n",
    "* The advantage of this approach is that once we have these parameters, we can use the same APIs that Nike's frontend uses - giving us access to clean, structured JSON data instead of having to parse messy HTML. This means more reliable data extraction and easier access to paginated results through API parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Category URLs] -->|Request| B[HTML Page]\n",
    "    B -->|Extract using BeautifulSoup| C[concept_ids]\n",
    "    B -->|Extract| D[path]\n",
    "    \n",
    "    C -->|Build API URL| E[API URL Constructor]\n",
    "    D -->|Build API URL| E\n",
    "    \n",
    "    E -->|Request| F[Product API]\n",
    "    F -->|Parse JSON Response| G[Product Data]\n",
    "    \n",
    "    G -->|More Pages?| H{Check next_page}\n",
    "    H -->|Yes| F\n",
    "    H -->|No| I[Save to CSV]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Category URLs\n",
    "URLS = [\n",
    "    \"https://www.nike.com/in/w/mens-shoes-nik1zy7ok\",\n",
    "    \"https://www.nike.com/in/w/womens-shoes-5e1x6zy7ok\",\n",
    "    \"https://www.nike.com/in/w/kids-shoes-v4dhzy7ok\",\n",
    "]\n",
    "\n",
    "CONSUMER_CHANNEL_ID = \"d9a5bc42-4b9c-4976-858a-f159cf99c647\"\n",
    "\n",
    "\n",
    "def get_headers(type=\"html\"):\n",
    "    \"\"\"Return appropriate headers based on request type\"\"\"\n",
    "    if type == \"html\":\n",
    "        return {\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "        }\n",
    "    else:  # API headers\n",
    "        return {\n",
    "            \"accept\": \"*/*\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9\",\n",
    "            \"nike-api-caller-id\": \"nike:dotcom:browse:wall.client:2.0\",\n",
    "            \"origin\": \"https://www.nike.com\",\n",
    "            \"referer\": \"https://www.nike.com/\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_concept_ids(session, url):\n",
    "    \"\"\"Extract concept IDs from Nike category page\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nFetching HTML from: {url}\")\n",
    "        response = session.get(url, headers=get_headers(\"html\"))\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        meta_tag = soup.find(\"meta\", {\"name\": \"branch:deeplink:$deeplink_path\"})\n",
    "        if not meta_tag or not meta_tag.get(\"content\"):\n",
    "            print(f\"No concept IDs found for {url}\")\n",
    "            return None\n",
    "\n",
    "        deeplink_path = meta_tag[\"content\"]\n",
    "        query_params = parse_qs(urlparse(deeplink_path).query)\n",
    "\n",
    "        if \"conceptid\" not in query_params:\n",
    "            print(f\"No concept IDs in meta tag for {url}\")\n",
    "            return None\n",
    "\n",
    "        return query_params[\"conceptid\"][0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting concept IDs for {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_response(response_json):\n",
    "    \"\"\"Parse API response and extract product information\"\"\"\n",
    "    try:\n",
    "        products_df = pd.json_normalize(\n",
    "            [\n",
    "                i.get(\"products\")[0]\n",
    "                for i in response_json[\"productGroupings\"]\n",
    "                if i.get(\"products\")\n",
    "            ]\n",
    "        )[\n",
    "            [\n",
    "                \"productCode\",\n",
    "                \"badgeLabel\",\n",
    "                \"copy.title\",\n",
    "                \"copy.subTitle\",\n",
    "                \"prices.currency\",\n",
    "                \"prices.currentPrice\",\n",
    "                \"pdpUrl.url\",\n",
    "                \"colorwayImages.portraitURL\",\n",
    "            ]\n",
    "        ]\n",
    "        return [products_df]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_nike_products(session, url, category):\n",
    "    \"\"\"Fetch all products for a given category URL\"\"\"\n",
    "    try:\n",
    "        headers = get_headers(\"api\")\n",
    "        print(f\"\\nFetching API data from: {url}\")\n",
    "\n",
    "        # Get initial response\n",
    "        initial_response = session.get(url, headers=headers)\n",
    "        data = initial_response.json()\n",
    "\n",
    "        total_products = data[\"pages\"][\"totalResources\"]\n",
    "        count = 100  # Maximum allowed count\n",
    "\n",
    "        # Calculate total pages properly\n",
    "        total_pages = ceil(total_products / count) - 1\n",
    "\n",
    "        # Initialize variables\n",
    "        all_products = []\n",
    "        next_page = data[\"pages\"][\"next\"]\n",
    "\n",
    "        # Parse and add initial products\n",
    "        initial_products = parse_response(data)\n",
    "        all_products.extend(initial_products)\n",
    "\n",
    "        # Create progress bar for pagination\n",
    "        with tqdm(\n",
    "            total=total_pages,  # Use calculated total pages\n",
    "            desc=f\"Fetching {category}\",\n",
    "            leave=False,\n",
    "            initial=1,  # Start from 1 since we've done initial request\n",
    "        ) as pbar:\n",
    "            while next_page:\n",
    "                # Random sleep between 1-3 seconds\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "\n",
    "                next_url = f\"https://api.nike.com{next_page}\"\n",
    "                print(f\"\\nFetching next page: {next_url}\")\n",
    "\n",
    "                # Make request for next page\n",
    "                response = session.get(next_url, headers=headers)\n",
    "                data = response.json()\n",
    "\n",
    "                # Parse and update products list\n",
    "                new_products = parse_response(data)\n",
    "                all_products.extend(new_products)\n",
    "\n",
    "                # Get next page URL before updating progress\n",
    "                next_page = data[\"pages\"].get(\"next\")\n",
    "\n",
    "                # Update progress only if there's more to fetch\n",
    "                if next_page:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # Combine all products and add category\n",
    "        if all_products:\n",
    "            df = pd.concat(all_products, ignore_index=True)\n",
    "            df[\"category\"] = category\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching products for {category}: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def construct_api_url(concept_ids, path):\n",
    "    \"\"\"Construct Nike API URL with concept IDs\"\"\"\n",
    "    base_url = \"https://api.nike.com/discover/product_wall/v1\"\n",
    "    params = {\"marketplace\": \"IN\", \"language\": \"en-GB\", \"count\": 100, \"anchor\": 0}\n",
    "\n",
    "    url = (\n",
    "        f\"{base_url}/marketplace/{params['marketplace']}/language/{params['language']}\"\n",
    "    )\n",
    "    url += f\"/consumerChannelId/{CONSUMER_CHANNEL_ID}\"\n",
    "    url += f\"?path={path}\"\n",
    "    url += f\"&attributeIds={concept_ids}\"\n",
    "    url += f\"&queryType=PRODUCTS&anchor={params['anchor']}&count={params['count']}\"\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def extract_path_from_url(url):\n",
    "    \"\"\"Extract the path component from Nike URL\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.path.lstrip(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`if count>100:`\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"error_id\": \"a834c280-65b8-4fb5-862c-a82fd9c7803b\",\n",
    "  \"errors\": [\n",
    "    {\n",
    "      \"code\": \"INCORRECT_COUNT_VALUE\",\n",
    "      \"message\": \"Value in field is not correct. Expected value is 24 or 50 or 100\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5bce48e62f4c8fae967108aeab0228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing categories:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching HTML from: https://www.nike.com/in/w/mens-shoes-nik1zy7ok\n",
      "No concept IDs found for https://www.nike.com/in/w/mens-shoes-nik1zy7ok\n",
      "\n",
      "Fetching HTML from: https://www.nike.com/in/w/womens-shoes-5e1x6zy7ok\n",
      "\n",
      "Fetching API data from: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=0&count=100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f31bbb5c9b450ca8ef2e2b7eee58c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching womens shoes:  20%|##        | 1/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=100&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=200&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=300&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=400&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/womens-shoes-5e1x6zy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,7baf216c-acc6-4452-9e07-39c2ca77ba32&queryType=PRODUCTS&anchor=500&count=100\n",
      "\n",
      "Fetching HTML from: https://www.nike.com/in/w/kids-shoes-v4dhzy7ok\n",
      "\n",
      "Fetching API data from: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=0&count=100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86b1eaae28442afaa6fda952cfdb189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching kids shoes:  20%|##        | 1/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=100&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=200&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=300&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=400&count=100\n",
      "\n",
      "Fetching next page: https://api.nike.com/discover/product_wall/v1/marketplace/IN/language/en-GB/consumerChannelId/d9a5bc42-4b9c-4976-858a-f159cf99c647?path=in/w/kids-shoes-v4dhzy7ok&attributeIds=16633190-45e5-4830-a068-232ac7aea82c,145ce13c-5740-49bd-b2fd-0f67214765b3&queryType=PRODUCTS&anchor=500&count=100\n",
      "\n",
      "Total products saved: 1175\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "session = requests.Session()\n",
    "# Create progress bar for categories\n",
    "with tqdm(total=len(URLS), desc=\"Processing categories\") as pbar:\n",
    "\n",
    "    for url in URLS:\n",
    "        category = \" \".join(url.split(\"/\")[-1].split(\"-\")[:-1])\n",
    "        # Get concept IDs\n",
    "        concept_ids = extract_concept_ids(session, url)\n",
    "        if not concept_ids:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # Extract path from URL\n",
    "        path = extract_path_from_url(url)\n",
    "\n",
    "        # Construct API URL\n",
    "        api_url = construct_api_url(concept_ids, path)\n",
    "\n",
    "        # Fetch products\n",
    "        df = fetch_nike_products(session, api_url, category)\n",
    "        if not df.empty:\n",
    "            all_data.append(df)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "# Combine all data\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(\"nike_products.csv\", index=False)\n",
    "    print(f\"\\nTotal products saved: {len(final_df)}\")\n",
    "else:\n",
    "    print(\"No data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Extracting amazon product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://www.amazon.in/NIKE-AIR-Winflo-11-6/dp/B0D7YTCKGS\",\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "with open(\"amazon1.html\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = curl_cffi_requests.get(\n",
    "    \"https://www.amazon.in/NIKE-AIR-Winflo-11-6/dp/B0D7YTCKGS\",\n",
    "    headers=headers,\n",
    ")\n",
    "with open(\"amazon2.html\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://tls.browserleaks.com/\", headers=headers)\n",
    "with open(\"requests.json\", \"w\") as f:\n",
    "    json.dump(response.json(), f, indent=4)\n",
    "\n",
    "response = curl_cffi_requests.get(\"https://tls.browserleaks.com/\", headers=headers)\n",
    "with open(\"curl_cffi_requests.json\", \"w\") as f:\n",
    "    json.dump(response.json(), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TLS fingerprinting is a security measure where websites identify browsers by their unique combination of TLS features (cipher suites, extensions, and settings) - similar to how each person has a unique fingerprint. Your browser automatically sends this during the HTTPS handshake.\n",
    "- curl-cffi doesn't match browser's TLS fingerprint exactly, but it provides a consistent, browser-like TLS implementation that includes critical features like proper cipher suites and HTTP/2 support. This makes it harder for websites to distinguish it from a real browser, unlike the requests library which has a very distinct fingerprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://curl-cffi.readthedocs.io/en/latest/faq.html#faq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Browser Request] --> B[TLS Fingerprint]\n",
    "    B --> C{Matches?}\n",
    "    \n",
    "    D[requests Library] --> E[Different JA3 Hash]\n",
    "    E --> C\n",
    "    \n",
    "    F[curl-cffi] --> G[Matching JA3 Hash]\n",
    "    G --> C\n",
    "    \n",
    "    C -->|No| H[Website Blocks]\n",
    "    C -->|Yes| I[Website Allows]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# amazon: pincode + scroll (via selenium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Try Requests Library] --> B[Blocked by Captcha]\n",
    "    B --> C[Try curl-cffi]\n",
    "    C --> D[Basic Scraping Works]\n",
    "    D --> E[Need Pincode Setting]\n",
    "    E --> F{Choose Approach}\n",
    "    F --> G[Replicate All Session Requests]\n",
    "    F --> H[Use Selenium/Browser]\n",
    "    \n",
    "    G -.->|Complex but Faster| I[Handle Multiple Requests]\n",
    "    H -.->|Simpler but Slower| J[Let Browser Handle Sessions]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the Chrome driver...\n",
      "\n",
      "Navigating to Amazon.in...\n",
      "Clicking on location button...\n",
      "Entering pincode: 560001\n",
      "Applying pincode...\n",
      "\n",
      "Navigating to product page: https://www.amazon.in/Jordan-Loyal-Shoes-White-RED-Black/dp/B0D6Z1Z4FG\n",
      "Scrolling to bottom of page...\n",
      "\n",
      "Navigating to product page: https://www.amazon.in/Nike-Mens-Revolution-Black-Running/dp/B0C8TH2TXS\n",
      "Scrolling to bottom of page...\n",
      "\n",
      "Navigating to product page: https://www.amazon.in/NIKE-Revolution-Mens-Running-Shoes/dp/B0D7MHTTJM\n",
      "Scrolling to bottom of page...\n",
      "\n",
      "Product Details:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIKE Mens Jordan Stay Loyal 3 Running Shoes</td>\n",
       "      <td>9163</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71slxw7HqT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike Mens Revolution 7Running Shoe</td>\n",
       "      <td>2993</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71lVk-4gvV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIKE Revolution 7 Men's Road Running Shoes (7)</td>\n",
       "      <td>3436</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81Qw4CuPc8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title  Price  \\\n",
       "0     NIKE Mens Jordan Stay Loyal 3 Running Shoes   9163   \n",
       "1              Nike Mens Revolution 7Running Shoe   2993   \n",
       "2  NIKE Revolution 7 Men's Road Running Shoes (7)   3436   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://m.media-amazon.com/images/I/71slxw7HqT...  \n",
       "1  https://m.media-amazon.com/images/I/71lVk-4gvV...  \n",
       "2  https://m.media-amazon.com/images/I/81Qw4CuPc8...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product Images:\n",
      "\n",
      "NIKE Mens Jordan Stay Loyal 3 Running Shoes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://m.media-amazon.com/images/I/71slxw7HqTL._SY695_.jpg\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nike Mens Revolution 7Running Shoe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://m.media-amazon.com/images/I/71lVk-4gvVL._SY695_.jpg\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NIKE Revolution 7 Men's Road Running Shoes (7):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://m.media-amazon.com/images/I/81Qw4CuPc8L._SX695_.jpg\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script completed successfully!\n",
      "\n",
      "Closing browser...\n"
     ]
    }
   ],
   "source": [
    "def setup_driver():\n",
    "    print(\"Setting up the Chrome driver...\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    time.sleep(2)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def update_location(driver, pincode):\n",
    "    print(\"\\nNavigating to Amazon.in...\")\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    print(\"Clicking on location button...\")\n",
    "    location_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"nav-global-location-popover-link\"))\n",
    "    )\n",
    "    location_button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"Entering pincode: {pincode}\")\n",
    "    pincode_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"GLUXZipUpdateInput\"))\n",
    "    )\n",
    "    pincode_input.clear()\n",
    "    time.sleep(1)\n",
    "    pincode_input.send_keys(pincode)\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"Applying pincode...\")\n",
    "    apply_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"GLUXZipUpdate\"]/span/input'))\n",
    "    )\n",
    "    apply_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "def extract_product_data(driver, product_url):\n",
    "    print(f\"\\nNavigating to product page: {product_url}\")\n",
    "    driver.get(product_url)\n",
    "    time.sleep(4)\n",
    "\n",
    "    print(\"Scrolling to bottom of page...\")\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for height in range(0, total_height, 100):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {height})\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Extract data using provided selectors\n",
    "    try:\n",
    "        title = (\n",
    "            WebDriverWait(driver, 10)\n",
    "            .until(EC.presence_of_element_located((By.ID, \"productTitle\")))\n",
    "            .text.strip()\n",
    "        )\n",
    "\n",
    "        price = (\n",
    "            WebDriverWait(driver, 10)\n",
    "            .until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"span.a-price-whole\"))\n",
    "            )\n",
    "            .text.strip()\n",
    "        )\n",
    "\n",
    "        image_url = (\n",
    "            WebDriverWait(driver, 10)\n",
    "            .until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"img#landingImage\"))\n",
    "            )\n",
    "            .get_attribute(\"src\")\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Price\": int(price.replace(\",\", \"\")),  # Convert price to integer\n",
    "            \"Image URL\": image_url,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = setup_driver()\n",
    "        update_location(driver, \"560001\")\n",
    "\n",
    "        # List of product URLs to scrape\n",
    "        product_urls = [\n",
    "            \"https://www.amazon.in/Jordan-Loyal-Shoes-White-RED-Black/dp/B0D6Z1Z4FG\",\n",
    "            \"https://www.amazon.in/Nike-Mens-Revolution-Black-Running/dp/B0C8TH2TXS\",\n",
    "            \"https://www.amazon.in/NIKE-Revolution-Mens-Running-Shoes/dp/B0D7MHTTJM\",\n",
    "        ]\n",
    "\n",
    "        # Collect data for all products\n",
    "        products_data = []\n",
    "        for url in product_urls:\n",
    "            data = extract_product_data(driver, url)\n",
    "            if data:\n",
    "                products_data.append(data)\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(products_data)\n",
    "\n",
    "        # Display table\n",
    "        print(\"\\nProduct Details:\")\n",
    "        display(df)\n",
    "\n",
    "        # Display images\n",
    "        print(\"\\nProduct Images:\")\n",
    "        for idx, row in df.iterrows():\n",
    "            print(f\"\\n{row['Title']}:\")\n",
    "            display(Image(url=row[\"Image URL\"], width=200))\n",
    "\n",
    "        print(\"\\nScript completed successfully!\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"\\nClosing browser...\")\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# A Step by Step Approach to Web Scraping ðŸ•·ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Part 1: Initial Setup & Request Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph InitialExploration[\"Initial Exploration\"]\n",
    "        A[Start] --> B{Check robots.txt & Terms}\n",
    "        B --> C{Sitemap/RSS Available?}\n",
    "        C -->|Yes| D[Use Structured Data Sources]\n",
    "        C -->|No| E[Manual Crawling Required]\n",
    "        E --> F{Check Page Loading}\n",
    "        F -->|Infinite Scroll| G[XHR Analysis Required]\n",
    "    end\n",
    "\n",
    "    subgraph RequestStrategy[\"Request Strategy Selection\"]\n",
    "        H{Anti-Bot Protection?}\n",
    "        H -->|Basic/None| I[Simple HTTP Requests]\n",
    "        I --> I1[requests/httpx + ThreadPoolExecutor + tenacity]\n",
    "        I --> I2[scrapy]\n",
    "        \n",
    "        H -->|TLS Fingerprinting| J[TLS Bypass Tools]\n",
    "        J --> J1[curl-cffi]\n",
    "        J --> J2[scrapy-impersonate]\n",
    "        \n",
    "        H -->|JavaScript Required| K[Browser Automation]\n",
    "        K --> K1[Selenium]\n",
    "        K --> K2[Playwright]\n",
    "        K --> K3[selenium-wire: if access to underlying requests is required]\n",
    "        K --> K4[undetected-chromedriver: to evade browser based anti-bot]\n",
    "    end\n",
    "\n",
    "    subgraph DataSourceAnalysis[\"Data Source Analysis\"]\n",
    "        L[Chrome DevTools]\n",
    "        M[Wappalyzer Chrome Extension]\n",
    "        L --> N[API Inspection]\n",
    "        N --> O1[curlconverter]\n",
    "        N --> O2[curl2scrapy]\n",
    "    end\n",
    "\n",
    "    F --> H\n",
    "    G --> H\n",
    "    D --> H\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Part 2: Execution & Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph DataExtraction[\"Data Extraction\"]\n",
    "        A{Data Format?}\n",
    "        A -->|HTML| B[HTML Parsing Tools]\n",
    "        B --> B1[BeautifulSoup]\n",
    "        B --> B2[Selectolax]\n",
    "        \n",
    "        A -->|JSON| C[JSON Tools]\n",
    "        C --> C1[JPath]\n",
    "    end\n",
    "\n",
    "    subgraph SessionManagement[\"Session Management\"]\n",
    "        D[requests.Session for Connection Reuse]\n",
    "        E[Captcha Service Integration]\n",
    "    end\n",
    "\n",
    "    subgraph ScalingPerformance[\"Scaling & Performance\"]\n",
    "        F[Proxy Rotation]\n",
    "        G[Distributed Scraping - scrapyd, selenium-grid]\n",
    "        H[Queue Management]\n",
    "    end\n",
    "\n",
    "    subgraph StorageProcessing[\"Storage & Processing\"]\n",
    "        I[Data Format Handling]\n",
    "        I --> I1[CSV]\n",
    "        I --> I2[JSON]\n",
    "        I --> I3[Database]\n",
    "        J[Error Handling & Retries]\n",
    "    end\n",
    "\n",
    "    subgraph Monitoring[\"Monitoring - spidermon\"]\n",
    "        K[Site Change Detection]\n",
    "        L[Performance Tracking]\n",
    "        M[Log Management]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    D --> F\n",
    "    F --> I\n",
    "    I --> K\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "| Tool/Resource | Category | Link |\n",
    "|--------------|----------|------|\n",
    "| Requests | HTTP Library | https://github.com/psf/requests |\n",
    "| HTTPX | HTTP Library | https://github.com/encode/httpx |\n",
    "| Tenacity | Rate Limiting | https://github.com/jd/tenacity |\n",
    "| Scrapy | Web Scraping Framework | https://github.com/scrapy/scrapy |\n",
    "| curl-cffi | TLS Fingerprint Bypass | https://github.com/lexiforest/curl_cffi |\n",
    "| scrapy-impersonate | TLS Fingerprint Bypass | https://github.com/jxlil/scrapy-impersonate |\n",
    "| Selenium | Browser Automation | https://github.com/SeleniumHQ/Selenium |\n",
    "| Playwright | Browser Automation | https://github.com/microsoft/playwright-python |\n",
    "| selenium-wire | Request Interception | https://github.com/wkeeling/selenium-wire |\n",
    "| curlconverter | Code Generation | https://curlconverter.com/ |\n",
    "| curl2scrapy | Code Generation | https://michael-shub.github.io/curl2scrapy/ |\n",
    "| BeautifulSoup | HTML Parsing | https://www.crummy.com/software/BeautifulSoup/bs4/doc/ |\n",
    "| Selectolax | HTML Parsing | https://github.com/rushter/selectolax |\n",
    "| XPath Practice | Learning Resource | https://topswagcode.dev/xpath/ |\n",
    "| CSS Practice | Learning Resource | https://flukeout.github.io/ |\n",
    "| JPath | JSON Parsing | https://thearmagan.github.io/JPATH/ |\n",
    "| Wappalyzer | Technology Detection | https://www.wappalyzer.com/ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
